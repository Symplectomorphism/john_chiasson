\section{Graph Shortest Path}
\label{sec:background}

\noindent These are the notes from the original paper~\cite{chen2021decision}.

The task is to find the shortest path on a \textit{fixed directed graph}. The 
\textbf{reward} is $0$ when the agent is at the goal and $-1$ otherwise. The 
\textbf{observation} is the integer index of the graph node the agent is in. The
\textbf{action} is the integer index of the graph node to move to next. The 
\textbf{transition dynamics} transport the agent to the action's node index if
there is an edge in the graph, while the agent remains at the past node
otherwise. The \textbf{returns-to-go} (i.e., the sum of future rewards),
abbreviated as \textit{rtg}, correspond to negative path lengths and maximizing 
them correspond to generating shortest paths.

We use a GPT model to generate both actions, observations (states), and 
returns-to-go tokens. This makes it possible for the model to generate its own 
(realizable) returns-to-go $\widehat{R}$. Since we require a return prompt to 
generate actions, and we do assume knowledge of the optimal path length upfront,
we use a simple prior over the returns that favor shorter paths: $P_{\text{prior}}(\widehat{R}=k) \propto T + 1 -k$, where $T$ is the maximum trajectory 
length. Then, it is combines with the return probabilities generated by the GPT 
model: 
%
\begin{align}
  \begin{split}
  P&(\widehat{R}_t \mid s_{0:t}, a_{0:t-1}, \widehat{R}_{0:t-1}) =  \\
&P_{\text{GPT}}(\widehat{R}=k \mid s_{0:t}, a_{0:t-1}, \widehat{R}_{0:t-1})
\times P_{\text{prior}}(\widehat{R}_t)^T.
  \end{split}
\end{align}
%
Note that the prior and return-to-go predictions are entirely computable by the 
model, and thus avoids the need for any external or oracle information like the optimal path length.

We train on a dataset of $1,000$ graph random walk trajectories of $T = 10$ 
steps, each with a random graph of $20$ nodes and edge sparsity coefficient of 
$0.1$.